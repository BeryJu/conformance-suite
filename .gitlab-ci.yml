# This file is a template, and might need editing before it works on your project.
# Auto DevOps
# This CI/CD configuration provides a standard pipeline for
# * building a Docker image (using a buildpack if necessary),
# * storing the image in the container registry,
# * running tests from a buildpack,
# * running code quality analysis,
# * creating a review app for each topic branch,
# * and continuous deployment to production
#
# In order to deploy, you must have a Kubernetes cluster configured either
# via a project integration, or via group/project variables.
# KUBE_INGRESS_BASE_DOMAIN must also be set as a variable at the group or project
# level, or manually added below.
#
# If you want to deploy to staging first, or enable canary deploys,
# uncomment the relevant jobs in the pipeline below.
#
# If Auto DevOps fails to detect the proper buildpack, or if you want to
# specify a custom buildpack, set a project variable `BUILDPACK_URL` to the
# repository URL of the buildpack.
# e.g. BUILDPACK_URL=https://github.com/heroku/heroku-buildpack-ruby.git#v142
# If you need multiple buildpacks, add a file to your project called
# `.buildpacks` that contains the URLs, one on each line, in order.
# Note: Auto CI does not work with multiple buildpacks yet

image: alpine:latest

variables:
  # KUBE_INGRESS_BASE_DOMAIN is the application deployment domain and should be set as a variable at the group or project level.
  # KUBE_INGRESS_BASE_DOMAIN: domain.example.com
  BASE_HOST_NAME: review-app
  PRODUCTION_HOST: www
  STAGING_HOST: staging
  DEMO_HOST: demo
  LM_JAVA_VERSION: 11
  SONAR_URL: "https://sonarqube.certification.openid.net"

stages:
  - build
  - review
  - demo
  - staging
  - canary
  - production
  - test
  - cleanup

include:
  - template: Security/Dependency-Scanning.gitlab-ci.yml
  - template: Security/License-Management.gitlab-ci.yml

build:
  stage: build
  image: docker:git
  services:
  - docker:dind
  variables:
    DOCKER_DRIVER: overlay2
  script:
    - setup_docker
    - install_build_dependencies
    - build
    - if [ -n "$GCP_PROJECT_ID" ]; then upload; fi
  artifacts:
    paths:
      - target/fapi-test-suite.jar
  cache:
    paths:
      - .m2/

test:
  stage: build
  image: gliderlabs/herokuish:latest
  script:
    # Run tests explicitly, since herokuish doesn't pick up the maven environment
    - cp -R * /tmp/build
    - "[ -e .m2 ] && cp -R .m2 /tmp/cache"
    - /tmp/buildpacks/05_buildpack-java/bin/test-compile /tmp/build /tmp/cache /tmp/env
    - export JAVA_HOME=/tmp/build/.jdk
    - HOME=/tmp/build source /tmp/build/.profile.d/maven.sh
    - cd /tmp/build
    - mvn -B test
    - >
      perl -ne '/Total.*?(\d+%)/ && print "Unit test coverage $1\n"' target/site/jacoco/index.html
  cache:
    paths:
      - .m2/
  only:
    - branches

check-trailing-whitespace:
  stage: build
  image: docker:git
  script:
    - apk -q add --no-cache --update python
    - ./scripts/checkwhitespace.py
  only:
    - branches

sonarqube_scanner:
  stage: build
  image: gliderlabs/herokuish:latest
  script:
    # Run tests explicitly, since herokuish doesn't pick up the maven environment
    - cp -R * /tmp/build
    - "[ -e .m2 ] && cp -R .m2 /tmp/cache"
    - /tmp/buildpacks/05_buildpack-java/bin/test-compile /tmp/build /tmp/cache /tmp/env
    - export JAVA_HOME=/tmp/build/.jdk
    - HOME=/tmp/build source /tmp/build/.profile.d/maven.sh
    - cd /tmp/build
    - run_sonaqube
  only:
    - branches

#codequality:
#  image: docker:latest
#  variables:
#    DOCKER_DRIVER: overlay2
#  services:
#    - docker:dind
#  script:
#    - setup_docker
#    - codeclimate
#  artifacts:
#    paths: [codeclimate.json]

review:
  stage: review
  script:
    - check_kube_domain
    - install_dependencies
    - download_chart
    - ensure_namespace
    - install_tiller
    - deploy
  environment:
    name: review/$CI_COMMIT_REF_NAME
    url: https://$BASE_HOST_NAME-$CI_COMMIT_REF_SLUG.$KUBE_INGRESS_BASE_DOMAIN
    on_stop: stop_review
  only:
    refs:
      - /^dev-branch-[1-9]$/
    kubernetes: active
  except:
    - master
    - production
    - demo

stop_review:
  stage: cleanup
  variables:
    GIT_STRATEGY: none
  script:
    - install_dependencies
    - delete
  environment:
    name: review/$CI_COMMIT_REF_NAME
    action: stop
  when: manual
  allow_failure: true
  only:
    refs:
      - /^dev-branch-[1-9]$/
    kubernetes: active
  except:
    - master
    - production
    - demo

demo:
  stage: demo
  script:
    - check_kube_domain
    - install_dependencies
    - download_chart
    - ensure_namespace
    - install_tiller
    - deploy
  environment:
    name: demo
    url: https://$DEMO_HOST.$KUBE_INGRESS_BASE_DOMAIN
  only:
    refs:
      - demo
    kubernetes: active

# Keys that start with a dot (.) will not be processed by GitLab CI.
# Staging and canary jobs are disabled by default, to enable them
# remove the dot (.) before the job name.
# https://docs.gitlab.com/ee/ci/yaml/README.html#hidden-keys

# Staging deploys are disabled by default since
# continuous deployment to production is enabled by default
# If you prefer to automatically deploy to staging and
# only manually promote to production, enable this job by removing the dot (.),
# and uncomment the `when: manual` line in the `production` job.

staging:
  stage: staging
  script:
    - check_kube_domain
    - install_dependencies
    - download_chart
    - ensure_namespace
    - install_tiller
    - deploy
  environment:
    name: staging
    url: https://$STAGING_HOST.$KUBE_INGRESS_BASE_DOMAIN
  only:
    refs:
      - master
    kubernetes: active

# Canaries are disabled by default, but if you want them,
# and know what the downsides are, enable this job by removing the dot (.),
# and uncomment the `when: manual` line in the `production` job.

.canary:
  stage: canary
  script:
    - check_kube_domain
    - install_dependencies
    - download_chart
    - ensure_namespace
    - install_tiller
    - deploy canary
  environment:
    name: production
    url: https://$PRODUCTION_HOST.$KUBE_INGRESS_BASE_DOMAIN
  when: manual
  only:
    refs:
      - master
    kubernetes: active

# This job continuously deploys to production on every push to `master`.
# To make this a manual process, either because you're enabling `staging`
# or `canary` deploys, or you simply want more control over when you deploy
# to production, uncomment the `when: manual` line in the `production` job.

production:
  stage: production
  script:
    - check_kube_domain
    - install_dependencies
    - download_chart
    - ensure_namespace
    - install_tiller
    - deploy
    - delete canary
  environment:
    name: production
    url: https://$PRODUCTION_HOST.$KUBE_INGRESS_BASE_DOMAIN
  only:
    refs:
      - production
    kubernetes: active

.deployment_test: &deployment_test
  stage: test
  image: python:alpine
  script:
    - check_kube_domain
    - install_dependencies
    - ensure_namespace
    - set_up_for_running_test_plan
    - run_all_test_plan

deployment_test_production:
  <<: *deployment_test
  environment:
    name: production
    url: https://$PRODUCTION_HOST.$KUBE_INGRESS_BASE_DOMAIN
  only:
    refs:
      - production
    kubernetes: active

deployment_test_staging:
  <<: *deployment_test
  environment:
    name: staging
    url: https://$STAGING_HOST.$KUBE_INGRESS_BASE_DOMAIN
  only:
    refs:
      - master
    kubernetes: active

deployment_test_demo:
  <<: *deployment_test
  environment:
    name: demo
    url: https://$DEMO_HOST.$KUBE_INGRESS_BASE_DOMAIN
  only:
    refs:
      - demo
    kubernetes: active

.deployment_test_review: &deployment_test_review
  stage: test
  image: python:alpine
  environment:
    name: review/$CI_COMMIT_REF_NAME
    url: https://$BASE_HOST_NAME-$CI_COMMIT_REF_SLUG.$KUBE_INGRESS_BASE_DOMAIN
    on_stop: stop_review
  only:
    refs:
      - /^dev-branch-[1-9]$/
    kubernetes: active

deployment_client_test_review:
  <<: *deployment_test_review
  script:
    - check_kube_domain
    - install_dependencies
    - ensure_namespace
    - set_up_for_running_test_plan
    - run_client_test_plan

deployment_server_test_review:
  <<: *deployment_test_review
  script:
    - check_kube_domain
    - install_dependencies
    - ensure_namespace
    - set_up_for_running_test_plan
    - run_server_test_plan

deployment_ciba_test_review:
  <<: *deployment_test_review
  script:
    - check_kube_domain
    - install_dependencies
    - ensure_namespace
    - set_up_for_running_test_plan
    - run_ciba_test_plan

# ---------------------------------------------------------------------------

.auto_devops: &auto_devops |
  # Auto DevOps variables and functions
  [[ "$TRACE" ]] && set -x
  export CI_APPLICATION_REPOSITORY=gcr.io/$GCP_PROJECT_ID/$CI_COMMIT_REF_SLUG
  export CI_APPLICATION_TAG=$CI_COMMIT_SHA
  export CI_CONTAINER_NAME=ci_job_build_${CI_JOB_ID}
  export MAVEN_CACHE=./.m2
  export TILLER_NAMESPACE=$KUBE_NAMESPACE

  function codeclimate() {
    cc_opts="--env CODECLIMATE_CODE="$PWD" \
             --volume "$PWD":/code \
             --volume /var/run/docker.sock:/var/run/docker.sock \
             --volume /tmp/cc:/tmp/cc"

    docker run ${cc_opts} codeclimate/codeclimate analyze -f json src > codeclimate.json
  }

  function deploy() {
    track="${1-stable}"
    name="$CI_ENVIRONMENT_SLUG"

    if [[ "$track" != "stable" ]]; then
      name="$name-$track"
    fi

    replicas="1"
    service_enabled="false"

    env_track=$( echo $track | tr -s  '[:lower:]'  '[:upper:]' )
    env_slug=$( echo ${CI_ENVIRONMENT_SLUG//-/_} | tr -s  '[:lower:]'  '[:upper:]' )

    if [[ "$track" == "stable" ]]; then
      # for stable track get number of replicas from `PRODUCTION_REPLICAS`
      eval new_replicas=\$${env_slug}_REPLICAS
      service_enabled="true"
    else
      # for all tracks get number of replicas from `CANARY_PRODUCTION_REPLICAS`
      eval new_replicas=\$${env_track}_${env_slug}_REPLICAS
    fi
    if [[ -n "$new_replicas" ]]; then
      replicas="$new_replicas"
    fi

    helm upgrade --install \
      --wait \
      --set application.env_slug="$CI_ENVIRONMENT_SLUG" \
      --set application.path_slug="$CI_PROJECT_PATH_SLUG" \
      --set service.enabled="$service_enabled" \
      --set releaseOverride="$CI_ENVIRONMENT_SLUG" \
      --set image.repository="$CI_APPLICATION_REPOSITORY" \
      --set image.tag="$CI_APPLICATION_TAG" \
      --set image.pullPolicy=IfNotPresent \
      --set application.track="$track" \
      --set service.url="$CI_ENVIRONMENT_URL" \
      --set replicaCount="$replicas" \
      --namespace="$KUBE_NAMESPACE" \
      --version="$CI_PIPELINE_ID-$CI_JOB_ID" \
      "$name" \
      chart/

    MONGO_POD=`kubectl -n ${KUBE_NAMESPACE} get pod -l app=mongodb -l release=${CI_ENVIRONMENT_SLUG} -o template --template="{{(index .items 0).metadata.name}}"`
    echo -n "Waiting for mongodb to start"
    while ! kubectl exec $MONGO_POD -- mongo mongodb://localhost/test_suite --eval "db.version()" > /dev/null 2>&1; do echo -n .; sleep 1; done; echo
    echo "Inserting CI token"
    CI_TOKEN=$( dd bs=64 count=1 < /dev/urandom | base64 - | tr -d '\n' )
    kubectl exec $MONGO_POD -- \
      mongo mongodb://localhost/test_suite --eval \
        "db.API_TOKEN.update({ _id: \"ci_token\" }, { _id: \"ci_token\", owner: { sub: \"ci\", iss: \"${CI_ENVIRONMENT_URL}\" }, info: {}, token: \"${CI_TOKEN}\", expires: null }, { upsert: true })"
  }

  function install_dependencies() {
    apk add -U openssl curl tar gzip bash ca-certificates git
    wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub
    wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.23-r3/glibc-2.23-r3.apk
    apk add glibc-2.23-r3.apk
    rm glibc-2.23-r3.apk

    curl https://kubernetes-helm.storage.googleapis.com/helm-v2.14.0-linux-amd64.tar.gz | tar zx
    mv linux-amd64/helm /usr/bin/
    helm version --client

    curl -L -o /usr/bin/kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
    chmod +x /usr/bin/kubectl
    kubectl version --client
  }

  function setup_docker() {
    if ! docker info &>/dev/null; then
      if [ -z "$DOCKER_HOST" -a "$KUBERNETES_PORT" ]; then
        export DOCKER_HOST='tcp://localhost:2375'
      fi
    fi
  }

  function download_chart() {
    if [[ ! -d chart ]]; then
      auto_chart=${AUTO_DEVOPS_CHART:-gitlab/auto-deploy-app}
      auto_chart_name=$(basename $auto_chart)
      auto_chart_name=${auto_chart_name%.tgz}
    else
      auto_chart="chart"
      auto_chart_name="chart"
    fi

    helm init --client-only
    helm repo add gitlab https://charts.gitlab.io
    if [[ ! -d "$auto_chart" ]]; then
      helm fetch ${auto_chart} --untar
    fi
    if [ "$auto_chart_name" != "chart" ]; then
      mv ${auto_chart_name} chart
    fi

    helm dependency update chart/
    helm dependency build chart/
  }

  function ensure_namespace() {
    kubectl describe namespace "$KUBE_NAMESPACE" || kubectl create namespace "$KUBE_NAMESPACE"
  }

  function check_kube_domain() {
    if [ -z ${KUBE_INGRESS_BASE_DOMAIN+x} ]; then
      echo "In order to deploy, KUBE_INGRESS_BASE_DOMAIN must be set as a variable at the group or project level, or manually added in .gitlab-ci.yml"
      false
    else
      true
    fi
  }

  function install_build_dependencies() {
    apk -q add --no-cache --update curl tar gzip make ca-certificates openssl python py-pip
    update-ca-certificates
    pip -q install docker-compose==1.23.2
    docker-compose --version

    curl https://dl.google.com/dl/cloudsdk/release/google-cloud-sdk.tar.gz | tar zx
    ./google-cloud-sdk/install.sh --usage-reporting=false --path-update=true
    ./google-cloud-sdk/bin/gcloud --quiet components update
  }

  function run_sonaqube() {
    mvn --batch-mode verify sonar:sonar -Dsonar.host.url=$SONAR_URL -Dsonar.login=$SONAR_LOGIN -Dsonar.analysis.mode=preview -Dsonar.gitlab.commit_sha=$CI_COMMIT_SHA -Dsonar.gitlab.ref_name=$CI_COMMIT_REF_NAME -Dsonar.gitlab.project_id=$CI_PROJECT_PATH -Dsonar.gitlab.unique_issue_per_inline=true -Dsonar.gitlab.url=https://gitlab.com -Dsonar.gitlab.user_token=$SONARQUBE_BOT_API_KEY -Dsonar.gitlab.max_blocker_issues_gate=0 -Dsonar.gitlab.json_mode=SAST -Dsonar.gitlab.failure_notification_mode=exit-code
  }

  function build() {
    docker-compose -f builder-compose.yml run builder
    docker build -t conformance-suite .
  }

  function upload() {
    docker tag conformance-suite "$CI_APPLICATION_REPOSITORY:$CI_APPLICATION_TAG"

    echo "Logging in to container registry..."
    echo "$GCLOUD_SERVICE_KEY" | base64 -d > ${HOME}/gcloud-service-key.json
    ./google-cloud-sdk/bin/gcloud auth activate-service-account --key-file ${HOME}/gcloud-service-key.json

    echo "Pushing to GCR..."
    ./google-cloud-sdk/bin/gcloud docker -- push "$CI_APPLICATION_REPOSITORY:$CI_APPLICATION_TAG"
    echo ""
  }

  function install_tiller() {
    echo "Checking Tiller..."
    helm init --upgrade
    kubectl rollout status -n "$TILLER_NAMESPACE" -w "deployment/tiller-deploy"
    if ! helm version --debug; then
      echo "Failed to init Tiller."
      return 1
    fi
    echo ""
  }

  function delete() {
    track="${1-stable}"
    name="$CI_ENVIRONMENT_SLUG"

    if [[ "$track" != "stable" ]]; then
      name="$name-$track"
    fi

    helm delete --purge "$name" || true
  }

  function set_up_for_running_test_plan() {
    echo "Installing extra dependencies"
    apk add -U openssh-client git
    apk add --update nodejs nodejs-npm
    pip install requests
    eval $(ssh-agent -s)
    echo "$SSH_PRIVATE_KEY" | ssh-add -
    cd ..
    GIT_SSH_COMMAND="ssh -o StrictHostKeyChecking=no" git clone git@gitlab.com:openid/conformance-suite-private.git
    cd conformance-suite-private
    export CONFORMANCE_SERVER="${CI_ENVIRONMENT_URL}/"
    MONGO_POD=`kubectl -n ${KUBE_NAMESPACE} get pod -l app=mongodb -l release=${CI_ENVIRONMENT_SLUG} -o template --template="{{(index .items 0).metadata.name}}"`
    export CONFORMANCE_TOKEN=$(kubectl -n ${KUBE_NAMESPACE} exec ${MONGO_POD} -- mongo --quiet mongodb://localhost/test_suite --eval "db.API_TOKEN.findOne({ _id: \"ci_token\" }).token")
  }

  function run_all_test_plan() {
    echo "Running automated tests against $CONFORMANCE_SERVER"
    ../conformance-suite/.gitlab-ci/run-tests.sh
  }

  function run_client_test_plan() {
    echo "Running automated tests against $CONFORMANCE_SERVER"
    ../conformance-suite/.gitlab-ci/run-tests.sh --client-tests-only
  }

  function run_server_test_plan() {
    echo "Running automated tests against $CONFORMANCE_SERVER"
    ../conformance-suite/.gitlab-ci/run-tests.sh --server-tests-only
  }

  function run_ciba_test_plan() {
      echo "Running automated tests against $CONFORMANCE_SERVER"
      ../conformance-suite/.gitlab-ci/run-tests.sh --ciba-tests-only
    }

before_script:
  - *auto_devops
